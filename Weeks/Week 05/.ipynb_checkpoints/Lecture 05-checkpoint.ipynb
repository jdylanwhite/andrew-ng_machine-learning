{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Neural Networks: Learning\n",
    "\n",
    "This time, we'll discuss how to *train* NNs. We will learn the \"backpropagation\" algorithm for training these models. The topics we'll discuss include:\n",
    "\n",
    "* Cost Function and Backpropagation\n",
    "  * Cost Function\n",
    "  * Backpropagation Algorithm\n",
    "  * Backpropagation Intuiting\n",
    "* Backpropagation in Practice\n",
    "  * Implementation Note: Unrolling Parameters\n",
    "  * Gradient Checking\n",
    "  * Random Initialization\n",
    "  * Putting it Together\n",
    "* Application of Neural Networks\n",
    "  * Autonomous Driving\n",
    "  \n",
    "## Cost Function and Backpropagation\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "NNs are one of the most powerful learning algorithms we have today. We learned a bit about them last week, but now we need to know how to fit the parameters for a training set. We'll start by talking about the cost function for fitting the parameters of the network.\n",
    "\n",
    "Suppose we have a training dataset\n",
    "\n",
    "$$ \\big\\{ (x^{(1)},y^{(1)}) , (x^{(2)},y^{(2)}) , \\cdots , (x^{(m)},y^{(m)}) \\big\\} $$\n",
    "\n",
    "with \n",
    "* $L$ number of layers in the network, and \n",
    "* $s_l$ number of units (not including the bias unit) in layer $l$\n",
    "\n",
    "We'll consider two types of classification; \n",
    "1. **binary classification**, where\n",
    "   $$ y \\in \\{0,1\\} $$\n",
    "   and there is 1 output unit, or\n",
    "   $$s_L = 1$$\n",
    "   We'll say, for simplicity that $K$, the number of classes, is 1.\n",
    "2. **multiclass classification** of $K$ classes, where\n",
    "   $$y \\in \\mathbb{R}^K$$\n",
    "   and there are $K$ output units, or \n",
    "   $$s_L = K, K \\ge 3$$ \n",
    "\n",
    "Our cost function for the NN will resemble a more broad version of the logistic regression cost function:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "J(\\Theta) = & - \\frac{1}{m} \\Big[ \\sum_{i=1}^m \\sum_{k=1}^K\n",
    "               y_k^{(i)} \\log \\big( (h_\\Theta (x^{(i)}))_k \\big) + \n",
    "               (1-y_k^{(i)}) \\log \\big( 1 - (h_\\Theta (x^{(i)}))_k \\big)\n",
    "               \\Big] \\\\\n",
    "               & + \\frac{\\lambda}{2 m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} \n",
    "               \\big( \\Theta_{j,i}^{(l)} \\big)^2\n",
    "\\end{align} \n",
    "\\\\ \n",
    "\\\\\n",
    "h_\\Theta (x) \\in \\mathbb{R}^K, (h_\\Theta(x))_i := i\\text{th output}\n",
    "$$\n",
    "\n",
    "Again, this resembles the logistic regression, where we're basically summing over multiple logistic regression cost functions. Our regularization term is similar again, but we're summing over the parameter matix (rather than array) for all of the layers.\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "\n",
    "The backpropagation algorithm is a method for minimizing the cost function on the paramters. To do so, we'll again need to compute, for a given set of parameters\n",
    "\n",
    "$$ J(\\Theta) \\text{ and } \\frac{\\partial}{\\partial \\Theta_{i,j}^{(l)}} J(\\Theta) $$\n",
    "\n",
    "Let's start by talking about a training set of only one sample,\n",
    "\n",
    "$$(x,y)$$\n",
    "\n",
    "and using a 4 layer NN. Our forward propagation looks as following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{(1)} & = x \\\\\n",
    "z^{(2)} & = \\Theta^{(1)} a^{(1)} \\\\\n",
    "a^{(2)} & = g(z^{(2)}), \\text{ add } a^{(2)}_0 \\\\\n",
    "z^{(3)} & = \\Theta^{(2)} a^{(2)} \\\\\n",
    "a^{(3)} & = g(z^{(3)}), \\text{ add } a^{(3)}_0 \\\\\n",
    "z^{(4)} & = \\Theta^{(3)} a^{(3)} \\\\\n",
    "a^{(4)} & = h_\\Theta(x) = g(z^{(4)}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now to compute the gradients, we need to use backpropagation. The intuition is \n",
    "- $\\delta_j^{(l)}$ : \"error\" of node $j$ in layer $l$ \n",
    "\n",
    "This is the \"error\" in the activation node in the layer. For L=4,\n",
    "\n",
    "$$ \\delta_j^{(4)} = a_j^{(4)} - y_j  = (h_\\Theta (x))_j - y_j .$$\n",
    "\n",
    "And now we need the earlier layer errors:\n",
    "\n",
    "$$ \\delta^{(3)} = (\\Theta^{(3)})^T \\delta^{(4)} \\cdot g^\\prime(z^{(3)}) \\\\\n",
    "   \\delta^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)} \\cdot g^\\prime(z^{(2)}) $$\n",
    "   \n",
    "The \"$\\cdot$\" here is element wise multiplication. To compute $g^\\prime(z^{(3)})$,\n",
    "\n",
    "$$ g^\\prime(z^{(3)}) = a^{(3)} \\cdot (1 - a^{(3)}) $$\n",
    "\n",
    "Note that there is no initial delta function, since the first layer is just our input.\n",
    "\n",
    "It turns out (it's possible, but rigorous to prove) that \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\Theta_{i,j}^{(l)}} J ( \\Theta ) = a_j^{(l)} \\delta_i^{(l+1)} $$\n",
    "\n",
    "Now let's put it all together. Let's go back to our training set\n",
    "\n",
    "$$ \\big\\{ (x^{(1)},y^{(1)}) , (x^{(2)},y^{(2)}) , \\cdots , (x^{(m)},y^{(m)}) \\big\\} $$\n",
    "\n",
    "First, we set\n",
    "\n",
    "$$ \\Delta_{i,j}^{(l)} = 0 $$\n",
    "\n",
    "which will be used to compute our derivative terms. Next we loop through our training samples and perform the following computations:\n",
    "\n",
    "> For $i = 1$ to $m$:\n",
    ">\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; Set $a^{(1)} = x^{(i)}$\n",
    ">\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; Perform forward propagation to compute $a^{(l)}$ for $l \\in \\{2,3,\\cdots,L\\}$\n",
    "> \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; Using $y^{(i)}$, compute $\\delta^{(L)} = a^{(L)} - y^{(i)}$\n",
    "> \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; Compute $\\delta^{(L-1)}, \\delta^{(L-2)}, \\cdots , \\delta^{(2)}$\n",
    "> \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; $\\Delta_{i,j}^{(l)} := \\Delta_{i,j}^{(l)} + a_j^{(l)} \\delta_i^{(l+1)}$\n",
    "\n",
    "The last step can be written in vectorized notation as \n",
    "\n",
    "$$\\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)} (a^{(l)})^T $$\n",
    "\n",
    "And now finally, we can compute, with regularization,\n",
    "\n",
    "$$ \\begin{align}\n",
    "D_{i,j}^{(l)} & := \\frac{1}{m} \\Delta_{i,j}^{(l)}+ \\lambda \\Theta_{i,j}^{(l)} \\text{ for } j \\neq 0 \\\\\n",
    "D_{i,j}^{(l)} & := \\frac{1}{m} \\Delta_{i,j}^{(l)} \\text{ for } j = 0 \\\\\n",
    "\\end{align} $$\n",
    "\n",
    "and these terms are our derivative terms that we seek to calculate.\n",
    "\n",
    "### Backpropagation Intuition\n",
    "\n",
    "Here we'll work a bit more of the mechanical steps of backpropagation. Note that\n",
    "* $\\delta_j^{(l)}$ is the \"error\" of cost of $a_j^{(l)}$\n",
    "\n",
    "Formally,\n",
    "\n",
    "$$ \\delta_j^{(l)} = \\frac{\\partial}{\\partial z_j^{(l)}} \n",
    "   \\Big( y^{(i)} \\log \\big( h_\\Theta (x^{(i)}) \\big) + (1 - y^{(i)}) \\log \\big(1 - h_\\theta(x^{(i)} \\big) \n",
    "   \\Big) $$\n",
    "   \n",
    "The back propagation algorithm is similar to the forward propagation. For example,\n",
    "\n",
    "$$ \\delta_2^{(2)} = \\Theta_{1,2}^{(3)} \\delta_1^{(3)} + \\Theta_{2,2}^{(3)} \\delta_2^{(3)} $$\n",
    "\n",
    "for backward propagation, and for forward propagation we had\n",
    "\n",
    "$$ z_1^{(3)} = \\Theta_{1,0}^{(2)} 1 + \\Theta_{1,1}^{(2)} a_1^{(2)} + \\Theta_{1,2}^{(2)} a_2^{(2)} $$\n",
    "\n",
    "Note that we ignored the bias unit. This all just depends on how you implement the alogrithm.\n",
    "\n",
    "## Back Bropagation in Practice\n",
    "\n",
    "### Implementation Note: Unrolling Parameters\n",
    "\n",
    "Now, let's talk about unrolling the parameters from matrices into vectors. Let's say we're passing the the cost function and gradients to a minimizing function. These minimizing functions assume the parameters and gradients are vectors, not matrices.\n",
    "\n",
    "Let's take the example of a model with\n",
    "\n",
    "$$ s_1 = 10, s_2 = 10, s_3 = 1 \\\\\n",
    "   \\Theta^{(1)} \\in \\mathbb{R}^{10 \\times 11} , \\Theta^{(2)} \\in \\mathbb{R}^{10 \\times 11}, \\Theta^{(3)} \\in \\mathbb{R}^{1 \\times 11} \\\\\n",
    "   S^{(1)} \\in \\mathbb{R}^{10 \\times 11} , S{(2)} \\in \\mathbb{R}^{10 \\times 11}, S{(3)} \\in \\mathbb{R}^{1 \\times 11} $$\n",
    "   \n",
    "So we'll have to make a vector containing *all* of the elements in *each* of the matrices so that we'd have\n",
    "\n",
    "$$ \\theta \\in \\mathbb{R}^{10 \\cdot 11 + 10 \\cdot 11 + 1 \\cdot 11} = \\mathbb{R}^{231} $$\n",
    "\n",
    "And then we'd have to reshape the vector such that the first matrix is the first 110 elements in the shape of 10 by 11, the second is the following 111 through 220 elements in the shape of 10 by 11, and the last 221 through 231 elements in the shape of 1 by 11. In Python, this whole process will involve using the `ravel` and `reshape` functions in NumPy. This will need to be done for the gradient matrices as well.\n",
    "\n",
    "### Gradient Checking\n",
    "\n",
    "Simplistically, we can think of gradient checking for a real number by estimating\n",
    "\n",
    "$$ \\frac{\\mathrm{d}}{\\mathrm{d}\\Theta} J ( \\Theta )\\approx \\frac{J(\\Theta + \\epsilon) - J(\\Theta - \\epsilon)}{2 \\epsilon} , \\epsilon \\ll 1$$\n",
    "\n",
    "Typically, we use a small value like $\\epsilon = 10^{-4}$.\n",
    "\n",
    "Now let's see what our approximation would look like for an unrolled parameter vector. What we'd have is \n",
    "\n",
    "$$ \\theta = [\\theta_1,\\theta_2,\\cdots,\\theta_n] \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_1} J(\\theta) \\approx \n",
    "\\frac{J(\\theta_1+\\epsilon,\\theta_2,\\cdots,\\theta_n)-J(\\theta_1-\\epsilon,\\theta_2,\\cdots,\\theta_n)}{2 \\epsilon} \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_2} J(\\theta) \\approx \n",
    "\\frac{J(\\theta_1,\\theta_2+\\epsilon,\\cdots,\\theta_n)-J(\\theta_1,\\theta_2-\\epsilon,\\cdots,\\theta_n)}{2 \\epsilon} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_n} J(\\theta) \\approx \n",
    "\\frac{J(\\theta_1,\\theta_2,\\cdots,\\theta_n+\\epsilon)-J(\\theta_1,\\theta_2,\\cdots,\\theta_n-\\epsilon)}{2 \\epsilon}$$\n",
    "\n",
    "We can then use this estimation to check the results of backwards propagation. This is really just a step for debugging, though. You don't want this running when you run the program because it can become very slow with gradient checking still on.\n",
    "\n",
    "### Random Initialization\n",
    "\n",
    "We have been initiating our parameters to zero. But now if all of our weights are the same, set to zero, then each activation layer element, and therefore each of our gradient elements, will all be equivalent. If the gradients are always equivalent, the gradient descent will continue to update the parameters, but they will still be completely identical, leaving again idetical hidden layer units. Since the hidden layer units in the second-to-last layer are being mapped to the single output, and each of our hidden layer units would be identical, this is the same as effectively mapping all of the input elements directly to one singular element, which is not very interesting. This is the problem of providing symmetric weights.\n",
    "\n",
    "In order to get around this problem, we initialize randomly. Programmatically, we will use the `numpy.random` module to get random arrays of a set size. We want to initialize each parameter as a random number between some small range\n",
    "\n",
    "$$ [ -\\tau , \\tau ] $$\n",
    "\n",
    "and the way we do that programmatically is something like the following, since the random array function returns values between the range of 0 and 1:\n",
    "\n",
    "    Theta1 = rand(10,11)*(2*tau)-tau\n",
    "    Theta2 = rand(1,11)*(2*tau)-tau\n",
    "\n",
    "\n",
    "### Putting It Together\n",
    "\n",
    "Now let's put all of the pieces together to see how to implement a NN learning algorithm.\n",
    "\n",
    "#### Pick an architecture\n",
    "\n",
    "The first thing you need to do is select a network achitecture. You may want to select a 3 element input layer to one hidden layer of 5 activation elements to a 4 class output layer. Or perhaps you want 2, or even 3 hidden layers.\n",
    "\n",
    "It is reasonable to start with just one hidden layer, and this is most common. It's also common, if you wish to use multiple hidden layers, to maintain the same number of activation units in each hidden layer. In general, having more hidden units is better, but it can be a bit more computationally expensive.\n",
    "\n",
    "Remember that if you have a multiclass output layer, you need to have outputs that are simply columns of the identity matrix. For example, if the training data output resembles\n",
    "\n",
    "$$ y \\in \\{ 1, 2, \\cdots, 10 \\} $$\n",
    "\n",
    "then $y=5$ needs to be mapped to \n",
    "\n",
    "$$ y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} $$ \n",
    "\n",
    "#### Training a NN\n",
    "\n",
    "The steps for training a NN are as follows:\n",
    "1. Randomaly initialize weights $\\Theta$\n",
    "2. Implement forward propagation to get $h_\\Theta(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement code to compute cost function $J(\\Theta)$\n",
    "4. Implement backpropagation to compute partial derivatives $\\frac{\\partial}{\\partial \\Theta_{j,k}^{(l)}} J (\\Theta)$\n",
    "5. Use gradient checking to compare the gradients found with backpropagation against a numerical estimate of the cost function gradient\n",
    "   Then after we've checked, we need to make sure to disable the gradient checking code.\n",
    "6. Use gradient descent or advanced optimization methods with backpropagation to minimize the cost function on the parameters.\n",
    "\n",
    "We usually do the forward and backward propagation with a `for` loop iterated over our taining samples. Also, the cost function here is non-convex for NNs, so minimization functions can sometimes get stuck in a local minimum. But usually this is not a problem. Even if it does get stuck, as long as the local minimum is somewhat low, the error should still also be low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
