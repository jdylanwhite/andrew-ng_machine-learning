{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Unsupervised Learning\n",
    "\n",
    "So far, we've covered some topics on *supervised* learning where we provide our training samples and then train a model on that dataset to predict on new data. Now we'll begin discussing *unsupervised* learning. For unsupervised learnings, we don't provided with the data labels, so we're not providing the training data. We'll also cover Principal Component Analysis which is used to speed up learning algorithms.\n",
    "\n",
    "The topics we'll cover this week are:\n",
    "* Unsupervised Learning\n",
    "  * Clustering\n",
    "    * Unsupervised Learning: Introduction\n",
    "    * K-Means Algorithm\n",
    "    * Optimization Objective\n",
    "    * Random Initialization\n",
    "    * Choosing the Number of Clusters\n",
    "* Dimensionality Reduction\n",
    "  * Motivation\n",
    "    * Motivation: Data Compression\n",
    "    * Motivation: Visualization\n",
    "  * Principal Component Analysis\n",
    "    * Problem Formulation\n",
    "    * Algorithm\n",
    "  * Applying PCA\n",
    "    * Reconstruction from a Compressed Representation\n",
    "    * Choosing the Number of PCs\n",
    "    * Advice\n",
    "    \n",
    "## Unsupervised Learning\n",
    "\n",
    "### Clustering\n",
    "\n",
    "#### Unsupervised Learning: Introduction\n",
    "\n",
    "In our supervised learning example, we provided training data along with labels,\n",
    "\n",
    "$$ \\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\cdots , (x^{(m)},y^{(m)}) \\}. $$\n",
    "\n",
    "In contrast, for unsupervised learning, we *only* have the data, with no labels,\n",
    "\n",
    "$$ \\{ x^{(1)}, x^{(2)}, \\cdots , x^{(m)} \\}. $$\n",
    "\n",
    "We give this dataset to an algorithm that then identifies labels on its own. An example is a clustering algorithm. Similar to the data in the previous assignment, we may have a linearly separable dataset, but instead of being labeled in one class or another, the unsupervised learning algorithm may identify separate clusters within the dataset. \n",
    "\n",
    "Some examples of clustering application problems:\n",
    "* market segmentation\n",
    "* social network analysis\n",
    "* organizing computing clusters\n",
    "* astronomical data analysis\n",
    "\n",
    "#### K-Means Algorithm\n",
    "\n",
    "The K-Means clustering algorithm is by far the most widely used algorithm for unsupervised clustering. Let's say we have a two-dimensional dataset composed of two clusters, such as the one in the previous assignment (but without the labels). \n",
    "\n",
    "![Data Array](./images/k-means_0.png \"Data Array\")\n",
    "\n",
    "The first step is to randomly choose two initial guesses for the cluster centroids.\n",
    "\n",
    "![Initial Guess](./images/k-means_0.1.png \"Initial Guess\")\n",
    "\n",
    "After this initial guess, there are two steps:\n",
    "1. Cluster Assignment \n",
    "2. Move the Centroid\n",
    "\n",
    "In the cluster assignment step, the K-means algorithm is going to iteratively assign each datapoint to the different cluster centroid based on how far away the points are from the initial cluster guess. \n",
    "\n",
    "![Step 1 - Cluster Assignment](./images/k-means_1.png \"Step 1 - Cluster Assignment\")\n",
    "\n",
    "Then, when it gets all of those points in the two clusters, the centroid is moved to the centroid of the newly formed clusters.\n",
    "\n",
    "![Step 2 - Move Centroid](./images/k-means_2.png \"Step 2 - Move Centroid\")\n",
    "\n",
    "And now we reiterate these two steps, several times, until we get the centroids to converge to the two separable clusters.\n",
    "\n",
    "![Final Clusters](./images/k-means_3.png \"Final Clusters\")\n",
    "\n",
    "More formally, the K-means algorithm takes as input\n",
    "* $K$, the number of clusters\n",
    "* Training set $\\{ x^{(1)} , x^{(2)}, \\cdots , x^{(m)} \\}$\n",
    "  \n",
    "  $$ x^{(i)} \\in \\mathbb{R}^n $$\n",
    "\n",
    "  where, by convention, we drop \n",
    "  \n",
    "  $$ x_0 = 1. $$\n",
    "  \n",
    "The first step is to randomly initialize our cluster centroids\n",
    "\n",
    "$$ \\mu_1, \\mu_2, \\cdots , \\mu_K \\in \\mathbb{R}^n, $$\n",
    "\n",
    "and then repeat\n",
    "\n",
    ">  for $i$=1 to $m$\n",
    ">\n",
    ">  &nbsp; &nbsp; $c^{(i)} :=$ index (from 1 to $K$) of cluster centroid closest to $x^{(i)}$\n",
    ">\n",
    ">  for $k$=1 to $K$\n",
    ">\n",
    ">  &nbsp; &nbsp; $\\mu_k :=$ average of points assigned to cluster $k$\n",
    "\n",
    "We can write\n",
    "\n",
    "$$ c^{(i)} = \\min_k ||x^{(i)} - \\mu_k||^2 .$$\n",
    "\n",
    "Note that we can use K-means for non-separated clusters as well to separate some data into clusters where it makes sense. One example is breaking up the non-separated spectrum of body sizes into clothing sizes. This is an example of market segmentation.\n",
    "\n",
    "#### Optimization Objective\n",
    "\n",
    "We're going to use one additional bit of notation:\n",
    "* $\\mu_{c^{(i)}} := $ cluster centroid of the cluster to which example $x^{(i)}$ has been assigned.\n",
    "\n",
    "Let's say, for example\n",
    "\n",
    "$$x^{(i)}$$\n",
    "\n",
    "has been assigned to clusuter number 5, which means\n",
    "\n",
    "$$c^{(i)} = 5$$\n",
    "\n",
    "which means\n",
    "\n",
    "$$\\mu_{c^{(i)}} = \\mu_5.$$\n",
    "\n",
    "With this notation, our optimization objective can be written\n",
    "\n",
    "$$J \\big( c^{(1)},\\cdots,c^{(m)},\\mu_1,\\cdots,\\mu_K \\big) = \\frac{1}{m} \\sum_{i=1}^m || x^{(i)} - \\mu_{c^{(i)}} ||^2 $$\n",
    "\n",
    "where we try to minimize as\n",
    "\n",
    "$$\\min_{c^{(1)},\\cdots,c^{(m)}, \\\\ \\mu_1,\\cdots,\\mu_K} J \\big( c^{(1)},\\cdots,c^{(m)},\\mu_1,\\cdots,\\mu_K \\big).$$\n",
    "\n",
    "This cost function is sometimes called the \"distortion\" function.  The step of assigning the clusters is the same as minimizing this cost function with respect to the cluster indices\n",
    "\n",
    "$$c^{(1)},\\cdots,c^{(m)}$$\n",
    "\n",
    "while holding the cluster centroids points\n",
    "\n",
    "$$\\mu^{(1)},\\cdots,\\mu^{(K)}$$\n",
    "\n",
    "the same.  Meanwhile the \"move centroid\" step is the same as minimizing the cluster centroid points\n",
    "\n",
    "$$\\mu^{(1)},\\cdots,\\mu^{(K)}$$\n",
    "\n",
    "while holding the cluster indices\n",
    "\n",
    "$$c^{(1)},\\cdots,c^{(m)}.$$\n",
    "\n",
    "the same. So what K-means is doing is partitioning our two variables an minimizing the cost function this way.\n",
    "\n",
    "#### Random Initialization\n",
    "\n",
    "We haven't yet discussed how to randomly initialize the cluster centroids. We need to choose\n",
    "\n",
    "$$ K < m $$\n",
    "\n",
    "and then randomly pick \n",
    "\n",
    "$$K$$\n",
    "\n",
    "individual training samples as our initial cluster centroids, and set\n",
    "\n",
    "$$ \\mu_1,\\cdots,\\mu_K $$\n",
    "\n",
    "K-means *can* get different solutions, and end up at local optima instead of the global optima. You can run K-means multiple times to try to get the best solution. A way to do that would be to run K means several, say 100, times, and then find the iteration with the lowest cost function.\n",
    "\n",
    "#### Choosing the Number of Clusters\n",
    "\n",
    "There's really not a great way to automatically choose the number of classes. It's usually done manually by using a bit of background knowledge about the data or the results you want. \n",
    "\n",
    "One method is the \"Elbow\" method. With this method, we'll run K-means with varying number of clusters and plot the cost function for each implementation. We'll find a curve that, ideally, will be steadily descending, but there will be an \"elbow\" where the rate of descent changes somewhat drastically, and this \"elbow\" is then our choice for the number of classes. However, fairly often, our curve is much more ambiguous, where the cost function just steadily drops for each new iteration. It's worth a shot, but don't have high expectations of it working for any particular problem.\n",
    "\n",
    "But sometimes we're running K-means for some purpose. For example, say we're separating body sizes into clusters for T-shirt sizes. We could break the class into three classes,\n",
    "* small, medium, or large\n",
    "\n",
    "or with five\n",
    "* extra-small, small, medium, larger, or extra-large\n",
    "\n",
    "But if we think of this from the perspective of the T-shirt business, it may make more sense to make more sizes to better fit your customers, or it may make more sense to limit production of multiple sizes. So in this case, it depends a bit more on what you want downstream of using the K-means clustering.\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "There are a couple of reasons we might want to do dimensionality reduction. One reason is data complression which allows us to use less memory or disk space but also speeds up our algorithm. Another is for visualization.\n",
    "\n",
    "One simple example of a reason to reduce dimensions is if we have one dimension that is length in inches, and another that is length (of the same thing) in centimenters. This is redundant, and we can get rid of one of these features. \n",
    "\n",
    "### Motivation\n",
    "\n",
    "#### Data Compression\n",
    "\n",
    "We can also reduce the dimensions when two features are highly correlated into one feature that combines the two.  One way to do this is by reducing the data from 2D to 1D. If we have a highly correlated, unique set of two features that requires two numbers to represent the correlations between the two features, we can actually just put them together as one number by combining the two and compressing that data. This halves the memory requirement for storing or manipulating this data.\n",
    "\n",
    "Similarly, we can reduce from 3D to 2D (at an exteme, we can reduce from 10,000 to 1,000). Here we could combine some 3D data onto a 2D plane and represent the data on a separate 2D feature space plane.\n",
    "\n",
    "#### Visualization\n",
    "\n",
    "Reducing dimensions also helps with visualizing data. Imagine we have data for various countries, as such\n",
    "\n",
    "Country | GDP (trill. USD) | Per capita GDP (thous. intl. D) | Human Development Index | Life Expenctancy | Poverty Index (Gini as percentage) | Mean House Income (thous. USD) | ...\n",
    "--- | --- | --- | --- | --- | --- | --- | ---\n",
    "Canada | 1.577 | 39.17 | 0.908 | 80.7 | 32.6 | 67.293 | ...\n",
    "China | 5.878 | 7.54 | 0.687 | 73 | 46.9 | 10.22 | ...\n",
    "India | 1.632 | 3.41 | 0.547 | 64.7 | 36.8 | 0.734 | ...\n",
    "Russia | 1.48 | 19.84 | 0.755 | 65.5 | 39.9 | 0.72 | ...\n",
    "Singapore | 0.223 | 56.69 | 0.866 | 80 | 42.5 | 67.1 | ...\n",
    "USA | 14.527 | 46.86 | 0.91 | 78.3 | 40.8 | 84.3 | ...\n",
    "... | ... | ... | ... | ... | ... | ... | ...\n",
    "\n",
    "Let's compare to a reduced version, performed by some dimenion reduction algorithm:\n",
    "\n",
    "Country | $z_1$ | $z_2$ \n",
    "--- | --- | ---\n",
    "Canada | 1.6 | 1.2\n",
    "China | 1.7 | 0.3\n",
    "India | 1.6 | 0.2\n",
    "Russia | 1.4 | 0.5\n",
    "Singapore | 0.5 | 1.7\n",
    "USA | 2 | 1.5\n",
    "... | ... | ...\n",
    "\n",
    "So now we've gone from many features to only two, making it easier to plot the data. Note that the dimension reduction algorithm may not necessarily prescribe any particular meeting to these new features.\n",
    "\n",
    "### Principal Component Analysis\n",
    "\n",
    "One of the most popular algorithms is principal component analysis, or PCA.\n",
    "\n",
    "#### Problem Formulations\n",
    "\n",
    "Formally, for a 2D to 1D reduction, we find a direction, or a vector\n",
    "\n",
    "$$ u^{(1)} \\in \\mathbb{R}^n ,$$\n",
    "\n",
    "onto which to project the 2D data to minimize the projection error. Generally, for reducing\n",
    "\n",
    "$$n \\rightarrow k$$\n",
    "\n",
    "dimensions, we find the vectors\n",
    "\n",
    "$$ u^{(1)}, u^{(2)}, \\cdots, u^{(k)} $$\n",
    "\n",
    "onto which to project the data so as to minimize the projection error. From linear algebra terminology, we are projecting the data onto the linear subspace that is spanned by these vectors.\n",
    "\n",
    "Despite some similarities, PCA *is not* linear regression. With linear regression, we're minimizing the *vertical* distance to the fitted line, but in PCA, we're minimizing the *line-normal* distance. More generally, with linear regressions, we have an output variable we're predicting, but this is not the case with PCA. There is no *special* feature that matters more than the other for PCA.\n",
    "\n",
    "**Note**: it's common to employ mean normalization and feature scaling prior to applying PCA.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "Before applying PCA, there's a data pre-processing step. We want to perform feature scaling/mean normalization. For a training set\n",
    "\n",
    "$$ \\mu_j = \\frac{1}{m} \\sum_{i=1}^m x_j^{(i)} $$\n",
    "\n",
    "and replace each\n",
    "\n",
    "$$ x_j^{(i)} $$\n",
    "\n",
    "with \n",
    "\n",
    "$$ x_j - \\mu_j $$\n",
    "\n",
    "And if different features are on different scales, scale them to have comparable ranges of values (by dividing by the standard deviation). This is the same as before, we're just applying this only to our unlabled data.\n",
    "\n",
    "The first thing we do is compute the \"covariance matrix\",\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)})(x^{(i)})^T \\in \\mathbb{R}^{n \\times n}$$\n",
    "\n",
    "and then we use singular value decomposition (SVD) to compute the eigenvectors of the covariance matrix. This is easily done with built-in linear algebra programs. We can use other methods to get the eigenvalues, but the SVD method tends to be a bit more stable with the covariance matrix. Our output from SVD will give three matrices\n",
    "\n",
    "$$U , S, V$$\n",
    "\n",
    "where we really want \n",
    "\n",
    "$$ U = \\begin{bmatrix} u^{(1)} | u^{(2)} | \\cdots | u^{(n)} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}.$$\n",
    "\n",
    "The column vectors of this matrix are our eigenvectors, and if we want to reduce our number of features, we just select the first features until we've selected out feature number of choice. So our reduced matrix will become\n",
    "\n",
    "$$ z = \\begin{bmatrix} u^{(1)} | u^{(2)} | \\cdots | u^{(k)} \\end{bmatrix}^T x \\in \\mathbb{R}^{k}, k \\leq n, $$\n",
    "\n",
    "where we call\n",
    "\n",
    "$$ U_\\text{reduce} = \\begin{bmatrix} u^{(1)} | u^{(2)} | \\cdots | u^{(k)} \\end{bmatrix} .$$\n",
    "\n",
    "To vectorize the computation of the covariance matrix, we compute\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{m} X^T X.$$\n",
    "\n",
    "### Applying PCA\n",
    "\n",
    "#### Reconstruction from Compressed Representation\n",
    "\n",
    "The should be a way to go from our compressed data back to our original uncompressed space. To go from\n",
    "\n",
    "$$ z \\in \\mathbb{R}^k \\rightarrow x \\in \\mathbb{R}^n $$\n",
    "\n",
    "we calculate\n",
    "\n",
    "$$ x_\\text{approx} = U_\\text{reduce} z $$\n",
    "\n",
    "and so long as the projection error was not too bad, this will be approximately the same as our original data. \n",
    "\n",
    "#### Choosing the number of PCs\n",
    "\n",
    "This number\n",
    "\n",
    "$$ k $$\n",
    "\n",
    "is a parameter of the PCA. How do we choose this number? The average squared projection error can be written\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{i=1}^m || x^{(i)} - x_\\text{approx}^{(i)} ||^2 ,$$\n",
    "\n",
    "and the total variation in the data can be written\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{i=1}^m || x^{(i)} ||^2 .$$\n",
    "\n",
    "A good rule is to choose \n",
    "\n",
    "$$ k : \\frac{\\frac{1}{m} \\sum_{i=1}^m || x^{(i)} - x_\\text{approx}^{(i)} ||^2}{\\frac{1}{m} \\sum_{i=1}^m || x^{(i)} ||^2} \\leq 0.01 ,$$\n",
    "\n",
    "or in other words, retain 99\\% of the variance. You can of course change this value, but it's a good idea to keep this number higher to retain the full scope provided by the original data. A way to achieve this is to incrementally step up the choice for the number of parameters and check if the rule was achieved. But there's a better way. The SVD provides another matrix,\n",
    "\n",
    "$$ S = \\begin{bmatrix}  s_{1,1} & & \\\\ & \\ddots & \\\\ & & s_{n,n} \\end{bmatrix} $$\n",
    "\n",
    "which is a purely diagonal matrix. It turnes out, the quantity above can be written instead as\n",
    "\n",
    "$$ 1 - \\frac{\\sum_{i=1}^k s_{i,i}}{\\sum_{i=1}^n s_{i,i}} \\leq 0.01 $$\n",
    "\n",
    "or of course\n",
    "\n",
    "$$ \\frac{\\sum_{i=1}^k s_{i,i}}{\\sum_{i=1}^n s_{i,i}} \\geq 0.99 .$$\n",
    "\n",
    "So with this, instead of running PCA multiple times to find the optimal number of pricipal components, we can run SVD only one time instead and then incrementally check the variance retained for different numbers with this one output matrix.\n",
    "\n",
    "#### Advice for applying PCA\n",
    "\n",
    "Here we'll talk about how to use PCA to speed up our learning algorithm. Let's discuss how to speed up a *supervised* learning algorithm with PCA. Let's say we're doing some computer vision work, and we're looking as an image that is 100 by 100 pixels, therfore giving 10,000 features per sample. This is a very large dataset to work with that PCA can help with. Running the learning algorithm on these 10,000 features will make our algorithm run quite slowly. So we'll take our data\n",
    "\n",
    "$$ \\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\cdots , (x^{(m)},y^{(m)}) \\} $$\n",
    "\n",
    "and extract our labels, giving\n",
    "\n",
    "$$ \\{ x^{(1)}, x^{(2)}, \\cdots , x^{(m)} \\} \\in \\mathbb{R}^{10000}. $$\n",
    "\n",
    "We now apply PCA and choose a smaller number of features, say 1000, to get\n",
    "\n",
    "$$ \\{ z^{(1)}, z^{(2)}, \\cdots , z^{(m)} \\}  \\in \\mathbb{R}^{1000} $$\n",
    "\n",
    "and we put this back together with  the labels to form a new dataset\n",
    "\n",
    "$$ \\{ (z^{(1)},y^{(1)}), (z^{(2)},y^{(2)}), \\cdots , (z^{(m)},y^{(m)}) \\} $$\n",
    "\n",
    "We then feed this data into our learning algorithm. If you have a new example to feed through the prediction you found with the learning algortihm, you run the example through the same mapping found from PCA, then plug that value into the hypothesis:\n",
    "\n",
    "$$ x_\\text{new} \\rightarrow z_\\text{new} \\rightarrow h_\\theta(z_\\text{new}) $$\n",
    "\n",
    "**Note**: Mapping with PCA should be defined by running PCA *only* on the training set, not the cross validation set, the test set, or new data for predictions.\n",
    "\n",
    "So we can use PCA to reduce memory/disk needed to store data or to speed up the learning algorithms by performing fewer computations. We also discussed using PCA to make visualization on higher order datasets more simple.\n",
    "\n",
    "There is one *bad* use of PCA: to prevent overfitting. The idea behind this is that if we use a much smaller number of features, we're less likely to overfit our data. This might actually work, but the better way to address overfitting is to use regularization instead.\n",
    "\n",
    "It's also sometimes used where it shouldn't be. Here's a common ML design:\n",
    "1. Get training set $ \\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\cdots , (x^{(m)},y^{(m)}) \\} $\n",
    "2. Run PCA to reduce $ x^{(i)} $ in dimensions to get $ z^{(i)} $\n",
    "3. Train logistic regression on $\\{ (z^{(1)},y^{(1)}), (z^{(2)},y^{(2)}), \\cdots , (z^{(m)},y^{(m)}) \\}$\n",
    "4. Test on test set $ \\{ (x_\\text{test}^{(1)},y_\\text{test}^{(1)}), (x_\\text{test}^{(2)},y_\\text{test}^{(2)}), \\cdots , (x_\\text{test}^{(m)},y_\\text{test}^{(m)}) \\} $\n",
    "   * Map  $ x^{(i)}_\\text{test} $ to  $z^{(i)}_\\text{test}$\n",
    "   * Run $h_\\theta(z)$ on $ \\{ (z_\\text{test}^{(1)},y_\\text{test}^{(1)}), (z_\\text{test}^{(2)},y_\\text{test}^{(2)}), \\cdots , (z_\\text{test}^{(m)},y_\\text{test}^{(m)}) \\} $\n",
    "   \n",
    "But what if we did the whole thing *without* using PCA first. That is, before implementing PCA, let's see if we want to do to with the original/raw data first. Only use PCA when it's needed. PCA is not always necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
