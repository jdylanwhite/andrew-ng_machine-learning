{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Multiclass Logistic Regression and Neural Networks\n",
    "\n",
    "In this exercise, you will implement one-vs-all logistic regression and neural\n",
    "networks to recognize hand-written digits.\n",
    "\n",
    "## Multiclass Classification\n",
    "\n",
    "We'll first tackle handwritten digit classification via logistic regression, implementing the one-vs-all classification technique for multiple classes. Each digit represents each digit, 0-9.\n",
    "\n",
    "### Loading the Data\n",
    "The training data is available as a matlab data file, so we have to read them in approprately. The training data contains 5000 samples, where each sample is a 20 by 20 pixel grayscale image of a single digit. Each pixel value is represented by a floating point number indicating the intensity of the pixel. As a result, our training data input matrix will be\n",
    "\n",
    "$$ X \\in \\mathbb{R}^{5000 \\times 400} $$\n",
    "\n",
    "and our output vector,\n",
    "\n",
    "$$ y \\in \\mathbb{R}^{5000} $$\n",
    "\n",
    "will hold the actual values of each training sample. Now let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy import optimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the data path\n",
    "dataDir = \"./assignment/ex3/\"\n",
    "dataFile = \"ex3data1.mat\"\n",
    "dataPath = dataDir+dataFile\n",
    "\n",
    "# Load the data\n",
    "data = loadmat(dataPath)\n",
    "X = data['X']\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Logistic Regression\n",
    "\n",
    "We'll use the one-vs-all logistic regression models, so we need to train 10 separate logistic regression classification. Since this requires a lot more computation that our previous assignments, we need to be sure our code is vectorized. Fortunately, I thought that was the goal all along and I've been vectorizing my code anyways, so we can utilize some of the code from the previous assignment. We'll create some functions here to evaluate the sigmoid function, calculate the cost and gradients, and to perform the BFGS minimization of the cost function.\n",
    "\n",
    "#### Sigmoid Function\n",
    "\n",
    "First, we need to create the sigmoid function that will calculate\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + \\exp(-z)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    \"\"\"Calculate the value of the sigmoid function for a given input\"\"\"\n",
    "    \n",
    "    # Return the sigmoid function\n",
    "    return (1.0 / (1.0 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost and Gradient, Regularized\n",
    "\n",
    "Next, we need to create functions to calculate the cost function and gradients to be used in our minimization. With regularization, our cost function is \n",
    "\n",
    "$$\n",
    "J (\\theta) = \\frac{1}{2 m} \\sum_{i=1}^m \\Big( h_\\theta (x^{(i)} - y^{(i)} \\Big)^2 + \\lambda \\sum_{j=1}^n \\theta_j^2\n",
    "$$\n",
    "\n",
    "And the regularized gradient is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\theta_0 & := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\Big( h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_0^{(i)} \\\\\n",
    "    \\theta_j & := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\Big( h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j,  \\text{ for } j \\in \\{1,2,\\cdots,n\\} \\\\\n",
    "    & := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m} \\sum_{i=1}^m \\Big( h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Recall that the minimization function will calculate the learning rate for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_cost(theta,X,y,regCoeff):\n",
    "    \n",
    "    \"\"\"Calculate the regularized cost function for logistic regression\"\"\"\n",
    "    \n",
    "    # Specify arrays as matrices\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).T\n",
    "    theta = np.matrix(theta).T\n",
    "    \n",
    "    # Get number of training samples\n",
    "    m = len(y)\n",
    "    \n",
    "    # Calculate the hypothesis function\n",
    "    h = sigmoid(X*theta)\n",
    "    \n",
    "    # Calculate the unregularized cost function\n",
    "    J = (1.0/m)*(-y.T*np.log(h) - (1.0-y).T*np.log(1.0-h))\n",
    "    \n",
    "    # Returen the regularized cost function \n",
    "    return (J + (regCoeff/(2.0*m))*(theta[1:].T*theta[1:])).item()\n",
    "\n",
    "def regularized_gradient(theta,X,y,regCoeff):\n",
    "    \n",
    "    \"\"\"Calculate the regularized gradient terms of the cost function\"\"\"\n",
    "   \n",
    "    # Specify arrays as matrices\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).T\n",
    "    theta = np.matrix(theta).T\n",
    "\n",
    "    # Get number of training samples\n",
    "    m = len(y)\n",
    "    \n",
    "    # Calculate the hypothesis function\n",
    "    h = sigmoid(X*theta)\n",
    "    \n",
    "    # Calculate the unregularized gradient\n",
    "    grad = (1.0/m)*(X.T*(h-y));\n",
    "    \n",
    "    # Return the regularize gradient\n",
    "    grad[1:] = grad[1:]+((regCoeff/m)*(theta[1:]));\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimization and One-vs-All\n",
    "\n",
    "Now we create a function to minimize the weights for the hypothesis and then another function to run this minimization on all of our classes. The results of this will be all of the weights for each of the classes. We can then use this to compute the prediction on a given input. \n",
    "\n",
    "I've seen a lot of people use the \"TNC\", or Truncated Newton Conjugate-Gradient method for minimizing a function, so I will use that as well. Note that the internal scipy code for each method is written differently, so if we want to change our method, we'll need to change some of our arrays to match the proper dimensions needed for that method's computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(cost,gradient,theta,args):\n",
    "\n",
    "    \"\"\"Use the scipy optimize BFGS minimization function to minimize the cost function\"\"\"\n",
    "    \n",
    "    # Set up the arguments\n",
    "    fun = cost\n",
    "    x0 = theta\n",
    "    fprime = gradient\n",
    "    args = args\n",
    "\n",
    "    # Return the minimized parameters\n",
    "    return optimize.minimize(fun=fun,x0=theta,args=args,method='TNC',jac=gradient)\n",
    "\n",
    "def one_vs_all(X,y,nLabels,regCoeff):\n",
    "\n",
    "    '''Run a minimization function on each class for a given training set'''\n",
    "    \n",
    "    # Set array size\n",
    "    m,n = X.shape\n",
    "\n",
    "    # Set array for the parameters of each of the classifiers\n",
    "    allTheta = np.zeros((nLabels,n+1))\n",
    "\n",
    "    # Insert a column of ones at the beginning for the intercept term\n",
    "    X = np.concatenate((np.ones((m,1)),X),axis=1)\n",
    "    y = y.ravel()\n",
    "\n",
    "    # Loop through labels 1-10\n",
    "    for i in range(1, nLabels+1):\n",
    "\n",
    "        # Set arrays for the minimization function\n",
    "        theta = np.zeros(n+1)\n",
    "        y_i = np.array([1 if label == i else 0 for label in y])\n",
    "\n",
    "        # Minimize the objective function\n",
    "        fmin = minimize(regularized_cost,regularized_gradient,theta,(X,y_i,regCoef))\n",
    "        allTheta[i-1,:] = fmin.x\n",
    "        \n",
    "    return allTheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Results\n",
    "\n",
    "To get the results of our classification, we need to calculate\n",
    "\n",
    "$$ \\Theta = \\texttt{allTheta} \\\\\n",
    "   h( X \\Theta^T ) = \\frac{1}{1 + \\exp(-X \\Theta^T)} $$\n",
    "   \n",
    "where our hypothesis gives a probability of the input beloging to each class. We then want to get the class for each input where the probability of that input is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(allTheta,X):\n",
    "\n",
    "    # Get X sizes\n",
    "    m,n = X.shape\n",
    "    \n",
    "    # Insert column of ones \n",
    "    X = np.insert(X, 0, values=np.ones(m), axis=1)\n",
    "    \n",
    "    # Convert to matrices\n",
    "    X = np.matrix(X)\n",
    "    allTheta = np.matrix(allTheta)\n",
    "    \n",
    "    # Compute the probability for each class on each training instance\n",
    "    h = sigmoid(X * allTheta.T)\n",
    "    \n",
    "    # Create array of the index with the maximum probability\n",
    "    hMax = np.argmax(h, axis=1)\n",
    "    \n",
    "    # Since our array was zero-indexed we need to add one for the true label prediction\n",
    "    hMax = hMax + 1\n",
    "    \n",
    "    return hMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Now let's implement the functions we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload X and y\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# Set learning and classification parameters\n",
    "regCoef = 0.1\n",
    "nLabels = 10\n",
    "\n",
    "# Get the weights from the minimization on the 10 layers\n",
    "allTheta = one_vs_all(X,y,nLabels,regCoef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Classes\n",
    "\n",
    "Let's now evaluate the highest probability classes for the training set and evaluate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 96.46%\n"
     ]
    }
   ],
   "source": [
    "# Run the prediction on the training set\n",
    "yPredicted = predict(allTheta,X)\n",
    "\n",
    "# Calculate the accuracy of the prediction to the true training samples\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(yPredicted, data['y'])]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print('Training accuracy = {0:0.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "In this part of the assignment, we'll use a neural network as our model. Our NN is shown below:\n",
    "\n",
    "![Our NN for Assignment 4](./assignment_nn.png \"Our NN for Assignment 4\")\n",
    "\n",
    "We are provided with the weights for the NN. We'll learn how to produce those weights next week.\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "Let's first load the data and the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data path\n",
    "dataDir = \"./assignment/ex3/\"\n",
    "dataFile = \"ex3data1.mat\"\n",
    "dataPath = dataDir+dataFile\n",
    "\n",
    "# Set the weights path\n",
    "weightsFile = \"ex3weights.mat\"\n",
    "weightsPath = dataDir+weightsFile\n",
    "\n",
    "# Load the data\n",
    "data = loadmat(dataPath)\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# Load the weights\n",
    "weights = loadmat(weightsPath)\n",
    "Theta1 = weights['Theta1']\n",
    "Theta2 = weights['Theta2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting from the Neural Network Weights\n",
    "\n",
    "Since we already have the weights, using the NN as a model is actually pretty simple.  We will first set\n",
    "\n",
    "$$ a_1 = [ 1_{m \\times 1} | X ] $$\n",
    "\n",
    "then just implement the following routine to get to the output, $a3$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_2 & = a_1 \\Theta_1^T \\\\\n",
    "a_2 & = [ 1_{s_2 \\times 1} | g (z_2) ] \\\\\n",
    "z_3 & = a_2 \\Theta_2^T \\\\\n",
    "a_3 & = g (z_3)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $s_i$ is the number of rows in $z_i$.\n",
    "\n",
    "### Implementation and Prediction\n",
    "\n",
    "Let's implement this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 97.52%\n"
     ]
    }
   ],
   "source": [
    "# Construct a1\n",
    "m = X.shape[0]\n",
    "a1 = np.concatenate((np.ones((m,1)),X),axis=1)\n",
    "\n",
    "# Calculate z2\n",
    "a1 = np.matrix(a1)\n",
    "Theta1 = np.matrix(Theta1)\n",
    "z2 = a1*Theta1.T\n",
    "\n",
    "# Calculate a3\n",
    "m = z2.shape[0]\n",
    "a2 = np.concatenate((np.ones((m,1)),sigmoid(z2)),axis=1)\n",
    "\n",
    "# Calculate z3\n",
    "a2 = np.matrix(a2)\n",
    "Theta2 = np.matrix(Theta2)\n",
    "z3 = a2*Theta2.T\n",
    "\n",
    "# Calculate a3\n",
    "a3 = sigmoid(z3)\n",
    "\n",
    "# Create array of the index with the maximum probability\n",
    "yPredicted = np.argmax(a3, axis=1)\n",
    "\n",
    "# Since our array was zero-indexed we need to add one for the true label prediction\n",
    "yPredicted = yPredicted + 1\n",
    "\n",
    "# Calculate the accuracy of the prediction to the true training samples\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(yPredicted, data['y'])]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print('Training accuracy = {0:0.2f}%'.format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
