{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Linear Regression with Multiple Variables\n",
    "This week, we expand upon the univariate linear regression covered last week to linear regressions on multiple variables.\n",
    "\n",
    "Topics for this week include\n",
    "* Multivariate Linear Regression\n",
    "* Computing Parameters Programatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "We can use a version of linear regression that's more powerful, one that works on multiple variables/features. In our previous examples, we had a single feature (the size of the house) and a single result (the price of the house). We can start to include more variables, such as the age of the house, the number of bedrooms, etc. These additional variables will be demarked\n",
    "\n",
    "$$\n",
    "x_1, x_2, x_3, ... , x_n\n",
    "$$\n",
    "\n",
    "where\n",
    "* $n$ will mean the number of variable/feautures\n",
    "* $m$ will mean the number of training samples\n",
    "\n",
    "Furthermore, we will use the following notation\n",
    "\n",
    "$$\n",
    "x_j^{(i)}\n",
    "$$\n",
    "\n",
    "to identify the $j$th sample of the $i$th training sample. Take the following example:\n",
    "\n",
    "| Size (sq ft) | Num. bedrooms | Num. floors | Age of home | Price ($1000) |\n",
    "|---|---|---|---|---|\n",
    "2104|5|1|45|460\n",
    "1416|3|2|40|232\n",
    "1534|3|2|30|315\n",
    "852|2|1|36|178\n",
    "\n",
    "Here, \n",
    "* $m$ = 4\n",
    "* $n$ = 4\n",
    "* $x^{(2)}$, the second training sample, would be\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "1416 \\\\ \n",
    "2 \\\\ \n",
    "2 \\\\ \n",
    "40\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "* $x_4^{(2)}$ = 40\n",
    "\n",
    "With 4 variables, we now have a hypothesis that looks like\n",
    "\n",
    "$$\n",
    "h_\\theta (x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\theta_4 x_4\n",
    "$$\n",
    "\n",
    "For convenience, we define\n",
    "\n",
    "$$\n",
    "x_0 = 1\n",
    "$$\n",
    "\n",
    "Now our feature variable becomes\n",
    "\n",
    "$$\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{n+1}\n",
    "$$\n",
    "\n",
    "and our parameter variable becomes\n",
    "\n",
    "$$\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{n+1}\n",
    "$$\n",
    "\n",
    "making our hypothesis easily written as\n",
    "\n",
    "$$\n",
    "h_\\theta (x) = \\theta^T x = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "A vectorized form is our theta update is:\n",
    "\n",
    "$$\n",
    "h_\\theta (X) = X \\theta \\\\\n",
    "\\theta := \\theta - \\frac{\\alpha}{m} X^T (X \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Multivariate Linear Regressions\n",
    "Our parameter space is now defined as the vector\n",
    "\n",
    "$$ \\theta \\in \\mathbb{R}^{n+1} $$\n",
    "\n",
    "so our cost function can be written as \n",
    "\n",
    "$$ \n",
    "J (\\theta) = \\frac{1}{2 m} \\sum_{i=1}^m \\Big( h_\\theta (x^{(i)}) - y^{(i)} \\Big)^2\n",
    "$$\n",
    "\n",
    "which can also be written\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "J (\\theta) & = \\frac{1}{2 m} \\sum_{i=1}^m \\Big( \\theta^T x^{(i)} - y^{(i)} \\Big)^2 \\\\\n",
    "& = \\frac{1}{2 m} \\sum_{i=1}^m \\Bigg( \\Big( \\sum_{j=1}^n \\theta_j x_j^{(i)} \\Big) - y^{(i)} \\Bigg)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The process for gradient descent is therefore (where parameters are simultaneously updated)\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n",
    "$$\n",
    "\n",
    "By plugging in $J(\\theta)$, our gradient descent algorithm becomes for\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m \\Big( h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "And again this process is completed until convergence.\n",
    "\n",
    "### Feature Scaling\n",
    "A tip for gradient descent is to ensure each of the parameters are of similar scale. This causes the gradient descent to converge more quickly since the step size taken in the simultaneous update is roughly the same for each parameter.\n",
    "\n",
    "For example, back to our house cost example, se we have \n",
    "* $x_1$ : size (0-2000 sq ft)\n",
    "* $x_2$ : number of bedrooms (1-5)\n",
    "\n",
    "Our two parameters here are of very different scales, so a tip is to scale the features so that they range from approximately -1 to 1. In this example, we can rescale as such:\n",
    "* $x_1$ : $\\frac{\\text{size}}{2000}$\n",
    "* $x_2$ : $\\frac{\\text{number of bedrooms}}{5}$\n",
    "\n",
    "This helps ensure that a step in the gradient descent is of similar size for the various parameters. This allows the algorithm to converge more quickly. It's mainly just important that the parameters are all scaled to the same order of magnitude.\n",
    "\n",
    "Another idea is to normalize by the mean. To do so, remap as follows:\n",
    "\n",
    "$$ x_i := \\frac{x_i - \\mu_i}{\\sigma_i} $$\n",
    "\n",
    "where \n",
    "* $\\mu_i$ is the mean value of the features\n",
    "* $\\sigma_i$ is the range or standard deviation of the features\n",
    "\n",
    "In our example, if our mean house size is 1000 sq ft. and our mean bedroom number is 2,\n",
    "* $x_1$ : $\\frac{\\text{size} - 1000}{2000}$\n",
    "* $x_2$ : $\\frac{\\text{number of bedrooms} - 2}{5}$\n",
    "This scales the range of parameters back to around -0.5 and 0.5.\n",
    "\n",
    "This doesn't have to be exact. We just need to parameters in the same order of magnitude.\n",
    "\n",
    "### Debugging & The Learning Rate\n",
    "It's useful to plot the calculated minimized cost function per iteration of the gradient descent to ensure that the cost is gradually descending after each iteration.\n",
    "\n",
    "Also note that the number of steps it may take to converge with gradient descent will vary greatly per problem and execution. So plotting the minimized cost function is a good way to do this.\n",
    "\n",
    "A useful rule for is to declare convergence\n",
    "when the cost function decreases by less than .001 in one iteration. It can sometimes be difficult though, depending on the problem. So agian, looking at the plot and finding when the curve is flattening is a good tip.\n",
    "\n",
    "If the curve is not decreasing but increasing or oscillating instead, it's a good indicator that your learning rate is too large.  If the learning rate is too small, the algorithm will converge very slowly. It's suggested to actually try a *range* of values to capture one that's too slow and one that's too large and find the best convergence values.\n",
    "\n",
    "### Features and Polynomial Regression\n",
    "Back to the housing example, suppose we have a hypothesis where we are considering only the \"frontage\" or the width of the plot of land on which the house sits, and the \"depth\", the length of the plot of land on which the house sits. We can actually for a new model then that uses the land area of the plot of land. So we've now reduced the number of features from two to one. However, perhaps now a multi-degree polynomial is a better fit that a simple line. Let's say we've chosen a cubic function as our hypothesis. Our hypothesis function now looks as such:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta (x) & = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 \\\\\n",
    "& = \\theta_0 + \\theta_1 (\\text{size}) + \\theta_2 (\\text{size})^2 + \\theta_3 (\\text{size})^3 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So we've set now\n",
    "* $x_1 = (\\text{size})$\n",
    "* $x_2 = (\\text{size})^2$\n",
    "* $x_3 = (\\text{size})^3$\n",
    "\n",
    "**Note**: since we've chosen our features and model like this, feature scaling is much more important now. This is because the exponents in the polynomial will greatly separate features of different scales and this could slow down the gradient descent much moreso than in the linear case.\n",
    "\n",
    "We have a lot of freedom in choosing our features. We could have set it to a quadratic polynomial:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta (x) & = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 \\\\\n",
    "& = \\theta_0 + \\theta_1 (\\text{size}) + \\theta_2 (\\text{size})^2  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But we have to think about what this implies. A quadratic is symmetric, so it may fit the training data available reliably well, but it may not be a great fit. We'd expect the price to increase with the size of the plot of land, and the minimization of the cost function would ensure that the quadratic regression closely fits the available training data by increasing with size, but since it's quadratic, some other part of the regression will be unrealistic. \n",
    "\n",
    "A better choice might be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta (x) & = \\theta_0 + \\theta_1 x_1 + \\theta_2 \\sqrt{x_2} \\\\\n",
    "& = \\theta_0 + \\theta_1 (\\text{size}) + \\theta_2 (\\sqrt{\\text{size}})  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The main point is that you need to think about the implications of the model you want to use, but you have a lot of freedom. Ultimately, we'll look at an algorithm that will find the best type of function for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Parameters Analytically - The Normal Equation\n",
    "The normal equation, for some linear regression problems, will give us a much better way to solve for the optimal value of the parameters in our hypothesis. So far the algorithm that we've been using for linear regression is gradient descent where in order to minimize the cost function we would take this iterative algorithm that takes many steps, multiple iterations of gradient descent to converge to the global minimum. In contrast, the normal equation would give us a method to solve for theta analytically, so that rather than needing to run this iterative algorithm, we can instead just solve for the optimal value for the paramets all at once, so that in basically one step you get to the optimal value.\n",
    "\n",
    "Let's look back at our example:\n",
    "\n",
    "| Size (sq ft) | Num. bedrooms | Num. floors | Age of home | Price ($1000) |\n",
    "|---|---|---|---|---|\n",
    "2104|5|1|45|460\n",
    "1416|3|2|40|232\n",
    "1534|3|2|30|315\n",
    "852|2|1|36|178\n",
    "\n",
    "So our matrix system is such that\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "1 & 2104 & 5 & 1 & 45 \\\\\n",
    "1 & 1416 & 3 & 2 & 40 \\\\\n",
    "1 & 1534 & 3 & 2 & 30 \\\\\n",
    "1 & 852 & 2 & 1 & 36\n",
    "\\end{bmatrix},\n",
    "y = \n",
    "\\begin{bmatrix}\n",
    "460 \\\\\n",
    "232 \\\\\n",
    "315 \\\\\n",
    "178\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here,\n",
    "* $X$ is an $m \\times (n+1)$\n",
    "* $y$ is an $m$-dimensional vector\n",
    "\n",
    "And, through some linear algebra steps that a bit more involved, we find:\n",
    "\n",
    "$$\n",
    "\\theta = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "In this method, we don't need to do feature scaling, since we're not taking incremental steps via gradient descent to get to an optimal value for our parameters.\n",
    "\n",
    "Here are some pros and cons of the two methods:\n",
    "\n",
    "Gradient Descent | Normal Equation\n",
    "--- | ---\n",
    "Need to choose $\\alpha$ | Don't need to choose $\\alpha$\n",
    "Needs many iterations | No need to iterate\n",
    "Works well even for large $n$ | Slow if $n$ is very large\n",
    "Don't have to compute matrix inverse | Have to compute $(X^T X)^{-1}$\n",
    "\n",
    "### Normal Equation and Noninveribility\n",
    "In the normal equation method, we have to compute\n",
    "\n",
    "$$\n",
    "(X^T X)^{-1},\n",
    "$$\n",
    "\n",
    "but what if it's non-invertible? This is rare, but when it occurs, there are often two causes of this:\n",
    "1. The features are redundant\n",
    "   * For example:\n",
    "     * $x_1$ = size in sq. ft.\n",
    "     * $x_2$ = size in sq. m\n",
    "2. Too many features ($m \\leq n$)\n",
    "   * Delete some features, or use regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
