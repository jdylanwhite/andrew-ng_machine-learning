{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 -  Large Scale Machine Learning\n",
    "\n",
    "This week, we'll cover large scale machine learning. Since ML works best when we have an abundance of data to leverage for training, knowing how to handle \"big data\" is a very sought after skill.\n",
    "\n",
    "The topics we'll cover are:\n",
    "* Gradient Descent with Large Datasets\n",
    "  * Learning with Large Datasets\n",
    "  * Stochastic Gradient Descent\n",
    "  * Mini-Batch Gradient Descent\n",
    "  * Stochastic Gradient Descent Convergence\n",
    "* Advanced Topics\n",
    "  * Online Learning\n",
    "  * Map Reduce and Data Parallelism\n",
    "  \n",
    "## Gradient Descent with Large Datasets\n",
    " \n",
    "### Learning with Large Datasets\n",
    "\n",
    "One of the reasons learning algorithms work better in the last decade is simply because of the large increase in the volume of data we're keeping and using. But why do we want such large datasets? We saw in one example where we classified between two confusable words (e.g., two and too), where a study (Banko and Brill, 2001) showed results that indicated that it wasn't necessarily the best algorithm that performed the best but the algorithm that used the most data.\n",
    "\n",
    "Let's say we have a large dataset with a training set of 100,000,000 samples. So our gradient descent update term for a linear regression is\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)}, $$\n",
    "\n",
    "and to perform this for each feature is going to take *a lot* of computation. We need to find a way to reduce the computation expense here. One thing we can consider is just training the model on a smaller dataset , say of 1,000 samples.  We can then use learning curves to see if adding more than this smaller set would help. From before, we found that if\n",
    "* we had high variance (larger gap between $J_\\text{cv}(\\theta)$ and $J_\\text{train}(\\theta)$) then increasing the training set size was helpful\n",
    "* we had high bias (very small gap between $J_\\text{cv}(\\theta)$ and $J_\\text{train}(\\theta)$) then increasing the training set size was not helpful\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "With large training sets, gradient descents as we've been performing them can be quite computationally expensive. Here, we'll take a look at a modification of gradient descent, called *stochastic* gradient descent that will allow us to scale these algorithms to large datasets.\n",
    "\n",
    "Suppose we're using gradient descent on a linear regression. We'll stick to linear regression to introduce stochastic gradient descent, but this is applicable to other learning algorithms. Recall\n",
    "\n",
    "$$ h_\\theta (x) = \\sum_{j=0}^n \\theta_j x_j \\\\\n",
    "   J_\\text{train} (\\theta) = \\frac{1}{2 m} \\sum_{i=0}^m \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big)^2 $$\n",
    "   \n",
    "and our gradient descent update to parameters was to repeat\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)}, j \\in \\{0,\\cdots,n\\} $$\n",
    "\n",
    "until convergence. Remember that gradient descent is simply starting as some initial point in the cost function and then slowly moving towards the minimum in the cost function. For a large dataset, this summation in the gradient descent update step will take a lot of calculations, and when we do it this way, we call it *batch* gradient descent. This term *batch* refers to the fact that we're looking at *all* of the training data.\n",
    "\n",
    "In contrast to this expensive batch gradient descent, we'll come up with a way that doesn't need to look at each example but instead only a single example. We'll write the cost function in a slightly different way:\n",
    "\n",
    "$$ \\mathrm{cost} \\big( \\theta,(x^{(i)},y^{(i)}) \\big) = \\frac{1}{2} \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big)^2 \\\\\n",
    "   J_\\text{train} = \\frac{1}{m} \\sum_{i=0}^m \\mathrm{cost} \\big( \\theta,(x^{(i)},y^{(i)}) \\big) $$\n",
    "   \n",
    "The steps of our stochastic gradient descent are then\n",
    "1. Randomly shuffle dataset\n",
    "2. Repeatedly perform (maybe 1-10 times), looping over each training sample individually,\n",
    "\n",
    "   $$ \\theta_j := \\theta_j - \\alpha \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)}, j \\in \\{0,\\cdots,n\\}, i \\in \\{1,\\cdots,m\\} $$\n",
    "   \n",
    "So our second step here is looping through and trying to fit only individual training examples. The random shuffling simply makes sure we visit the examples in no particular order. We've now removed the need to sum over all taining examples. \n",
    "\n",
    "One consequence here is that we won't be guaranteed to always step closer to the minimum. Instead, we may sometimes step away from the minimum, but we'll still move in the general direction. However, once the gradient descent has reached the vicinity of the minimum, it will simply randomly wonder around the minimum rather than converging. But this isn't much of a problem, since we really only care about the cost function being small, not being right at the minimum. \n",
    "\n",
    "With large enough datasets, we may find that only passing though the gradient descent once is enough. This is because, if our dataset is large enough, we've made sufficiently many steps in gradient descent for each training sample.\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "   \n",
    "There's another variation on batch gradient descent: *mini-batch* gradient descent. Our three choices so far will then be\n",
    "* batch gradient descent: use all $m$ examples in each iteration\n",
    "* stochastic gradient descent: use 1 example in each iteration\n",
    "* mini-batch gradient descent: use $b<m$ examples in each iteration\n",
    "  * $b$ here is called the \"mini-batch\" size, and a typical range of mini-batch sizes is from 2-100\n",
    "\n",
    "Basically, we'll choose some mini-batch size, say\n",
    "\n",
    "$$ b = 10 $$\n",
    "\n",
    "and the get 10 examples\n",
    "\n",
    "$$ (x^{(i)},y^{(i)}), \\cdots, (x^{(i+9)},y^{(i+9)}) $$\n",
    "\n",
    "and update parameters via\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\frac{\\alpha}{10} \\sum_{i=1}^{i+9} \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)}, j \\in \\{0,\\cdots,n\\} $$\n",
    "\n",
    "and then we'd perform this for every 10 training sets. For example, say\n",
    "\n",
    "$$ b =10, m=1000 .$$\n",
    "\n",
    "Then our algorithm would be\n",
    "\n",
    "> Repeat {\n",
    "> \n",
    "> &nbsp; &nbsp; for $i = 1,11,21,31,\\cdots,991$ {\n",
    ">\n",
    "> &nbsp; &nbsp; &nbsp; &nbsp; $\\theta_j := \\theta_j - \\frac{\\alpha}{10} \\sum_{i=1}^{i+9} \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)}, j \\in \\{0,\\cdots,n\\}$\n",
    ">\n",
    "> &nbsp; &nbsp; }\n",
    ">\n",
    "> }\n",
    "\n",
    "This allows us to make progress on the gradient descent steps without having to through *all* of the samples for each update, as in full batch gradient descent. But when does mini-batch outperform stochastic gradient descent? In general, mini-batch only works best when we're implementing vectorization. So the summation we have in the gradient descent in mini-batch can be parallelized when linear algebra libraries can work with vectorized formulation, allowing better performance. \n",
    "\n",
    "\n",
    "One disadvantage of mini-batch gradient descent is that we now have a new parameter to consider, the mini-batch size.\n",
    "\n",
    "### Stochastic Gradient Descent Convergence\n",
    "\n",
    "How do we make sure the stochastic gradient descent is converging, and how to we choose our learning rate? Before, we were plotting our cost function\n",
    "\n",
    "$$ J_\\text{train}(\\theta) $$\n",
    "\n",
    "as a function of the number of iterations of gradient descent, and we'd make sure that the cost function was decreasing with each iteration. So now what we can do instead is compute, before updating the parameters and while it's scanning through our training samples,\n",
    " \n",
    "$$ \\mathrm{cost} \\big( \\theta,(x^{(i)},y^{(i)}) \\big), \\text{ using } (x^{(i)},y^{(i)}) $$\n",
    "\n",
    "So for a specific example, we're going to check to see how our gradient descent is doing for a specific example. And then we can plot this cost averaged over, say, 1000 examples, and do this for every 1000 iterations. Since on average, our stochastic gradient descent ought to be moving in general towards the minimum of the cost function. A smaller learning rate will likely make the oscillations seen in the plot a bit smaller. The larger number that we choose to average over, the smoother our curve will look, but we suffer from getting few points to plot. You may see the plot increase with iteration, and assuming you're code is bug-free, try using a smaller learning rate.\n",
    "\n",
    "As we have it now, stochastic gradient descent will ideally get near the minimum but never quite converge. Typically, we hold the learning rate constant. But we can slowly decrease the learning rate instead in order to get convergence. For example, we could set\n",
    "\n",
    "$$ \\alpha = \\frac{\\text{constant}_1}{\\text{iteration count} + \\text{constant}_2} .$$\n",
    "\n",
    "So here we have a learning rate that drops with each iteration count. One reason not to do this is of course becuase we now have two parameters. What we should find though is that, as we iterate, the gradient descent meanders towards the minimum, where the steps decrease as we move closer to the minimum since we're taking more iterations.\n",
    "\n",
    "## Advanced Topics\n",
    "\n",
    "### Online Learning\n",
    "\n",
    "We sometimes need to continuously take newly generated data. For example, suppose we run a website for a shipping service where a user specifies an origin and destination, and we offer to ship for a certain price. Sometimes, users will choose to use our shipping server, i.e.,\n",
    "\n",
    "$$ y = 1 ,$$\n",
    "\n",
    "and sometimes they will not, i.e.,\n",
    "\n",
    "$$ y = 0 .$$\n",
    "\n",
    "We want to learn on these features to optimize the price that we charge for shipping. Our features will capture properties of the user, origin/destination, and the asking price. So we want to learn the probability that they will elect to choose our service for a given price:\n",
    "\n",
    "$$ p(y=1|x;\\theta) .$$\n",
    "\n",
    "We can use a logistic regression here. What our online logistic regession will do is as follows:\n",
    "\n",
    "> Repeat forever {\n",
    "> \n",
    "> &nbsp; &nbsp; Get $(x,y)$ corresponding to user\n",
    "> \n",
    "> &nbsp; &nbsp; Update $\\theta$ using $(x,y)$\n",
    "> \n",
    "> &nbsp; &nbsp; &nbsp; &nbsp; $\\theta_j := \\theta_j - \\alpha \\big( h_\\theta (x) - y \\big) x_j,  j \\in \\{0,\\cdots,n\\} $\n",
    ">\n",
    "> }\n",
    "\n",
    "Here, we're going to just be using one sample at a time, so we're discarding our past examples and only updating for each user that accesses our site. This is a pretty good algorithm provided we have a continuous stream of users providing the data. \n",
    "\n",
    "One advantage here is that our algorithm can adapt to user preferences. So if something external happens (e.g., economy crashes) that makes people change their preferences for what they're willing to pay, we can adapt to that with such and algorithm.\n",
    "\n",
    "Let's look at another example, this time for product search. Say a user searches for \"Android phone 1080p camera\", and that we have 100 phones in our store, and we want to return 10 results for this search. Here's what we'll do.\n",
    "* Define $x$ as features of the phone, how many words match the user's query, etc.\n",
    "* Define $y=1$ if user clicks link, $y=0$ otherwise\n",
    "* Learn $p(y=1|x;\\theta)$\n",
    "* Use $p$ to show the user the top 10 phones they're most likely to click on\n",
    "\n",
    "This problem name is learning the \"predicted Click-Through Rate\" (CRT). Since we're showing 10 search results, for each search, we get 10 pairs\n",
    "\n",
    "$$(x,y)$$\n",
    "\n",
    "that we can learn from.\n",
    "\n",
    "### Map Reduce and Data Parallelism\n",
    "\n",
    "Some machine learning problems are too big to run on one computer, no matter what algorithm we choose. We can use \"map-reduce\" to use multiple machines to learn. Say we have, for simplicity, 400 training samples. Our batch gradient descent would be\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\frac{\\alpha}{400} \\sum_{i=1}^{400} \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)}, j \\in \\{0,\\cdots,n\\} $$\n",
    "\n",
    "Let's assume we have 4 computers to run in parallel. Machine 1 will use\n",
    "\n",
    "$$ \\{ (x^{(1)},y^{(1)}), \\cdots, (x^{(100)},y^{(100)}) \\} $$\n",
    "\n",
    "and calculate\n",
    "\n",
    "$$ \\text{temp}_j^{(1)} = \\sum_{i=1}^{100} \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)} $$\n",
    "\n",
    "Machine 2 will use\n",
    "\n",
    "$$ \\{ (x^{(101)},y^{(101)}), \\cdots, (x^{(200)},y^{(200)}) \\} $$\n",
    "\n",
    "and calculate\n",
    "\n",
    "$$ \\text{temp}_j^{(2)} = \\sum_{i=101}^{200} \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)} $$\n",
    "\n",
    "Machine 3 will use\n",
    "\n",
    "$$ \\{ (x^{(201)},y^{(201)}), \\cdots, (x^{(300)},y^{(300)}) \\} $$\n",
    "\n",
    "and calculate\n",
    "\n",
    "$$ \\text{temp}_j^{(3)} = \\sum_{i=201}^{300} \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)} $$\n",
    "\n",
    "And finally, Machine 4 will use\n",
    "\n",
    "$$ \\{ (x^{(301)},y^{(301)}), \\cdots, (x^{(400)},y^{(400)}) \\} $$\n",
    "\n",
    "and calculate\n",
    "\n",
    "$$ \\text{temp}_j^{(4)} = \\sum_{i=301}^{400} \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)} $$\n",
    "\n",
    "So now, each machine only has to do a quarter of the work, and presumably this step only takes a quarter of the time. The next step is to combine the results to update our parameters:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\frac{\\alpha}{400}  \\big( \\text{temp}_j^{(1)} + \\text{temp}_j^{(2)} + \\text{temp}_j^{(3)} + \\text{temp}_j^{(4)} \\big), j \\in \\{0,\\cdots,n\\} .$$\n",
    "\n",
    "Many learning algorithms can be expressed as computing sums of function over the training set, so it's often possible to do use map-reduce. For example, if we want to used an advanced optimization with logisitic regression. We need to compute the cost function and gradient:\n",
    "\n",
    "$$ J_\\text{train}(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)} \\log h_\\theta (x^{(i)}) - (1-y^{(i)}) \\log (1 - h_\\theta (x^{(i)})) \\\\ \n",
    "\\frac{\\partial}{\\partial \\theta_j} J_\\text{train}(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\Big(h_\\theta (x^{(i)}) - y^{(i)} \\Big) x_j^{(i)} $$\n",
    "\n",
    "We can again break these summations up across our machines, so we're still able to use map-reduce with the advanced optimization techniques.\n",
    "\n",
    "Note that you don't necessarily need different computers, but different computing cores. So if you have a quad-core CPU, then you can use map-reduce on your multi-core machine to parallelize. Some linear algebra libraries take advantage of this for you behind the scenes if you're using vectorized computations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
