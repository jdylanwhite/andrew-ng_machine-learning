{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Neural Networks: Learning\n",
    "\n",
    "This time, we'll discuss how to *train* NNs. We will learn the \"backpropagation\" algorithm for training these models. The topics we'll discuss include:\n",
    "\n",
    "* Cost Function and Backpropagation\n",
    "  * Cost Function\n",
    "  * Backpropagation Algorithm\n",
    "  * Backpropagation Intuiting\n",
    "* Backpropagation in Practice\n",
    "  * Implementation Note: Unrolling Parameters\n",
    "  * Gradient Checking\n",
    "  * Random Initialization\n",
    "  * Putting it Together\n",
    "* Application of Neural Networks\n",
    "  * Autonomous Driving\n",
    "  \n",
    "## Cost Function and Backpropagation\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "NNs are one of the most powerful learning algorithms we have today. We learned a bit about them last week, but now we need to know how to fit the parameters for a training set. We'll start by talking about the cost function for fitting the parameters of the network.\n",
    "\n",
    "Suppose we have a training dataset\n",
    "\n",
    "$$ \\big\\{ (x^{(1)},y^{(1)}) , (x^{(2)},y^{(2)}) , \\cdots , (x^{(m)},y^{(m)}) \\big\\} $$\n",
    "\n",
    "with \n",
    "* $L$ number of layers in the network, and \n",
    "* $s_l$ number of units (not including the bias unit) in layer $l$\n",
    "\n",
    "We'll consider two types of classification; \n",
    "1. **binary classification**, where\n",
    "   $$ y \\in \\{0,1\\} $$\n",
    "   and there is 1 output unit, or\n",
    "   $$s_L = 1$$\n",
    "   We'll say, for simplicity that $K$, the number of classes, is 1.\n",
    "2. **multiclass classification** of $K$ classes, where\n",
    "   $$y \\in \\mathbb{R}^K$$\n",
    "   and there are $K$ output units, or \n",
    "   $$s_L = K, K \\ge 3$$ \n",
    "\n",
    "Our cost function for the NN will resemble a more broad version of the logistic regression cost function:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "J(\\Theta) = & - \\frac{1}{m} \\Big[ \\sum_{i=1}^m \\sum_{k=1}^K\n",
    "               y_k^{(i)} \\log \\big( (h_\\Theta (x^{(i)}))_k \\big) + \n",
    "               (1-y_k^{(i)}) \\log \\big( 1 - (h_\\Theta (x^{(i)}))_k \\big)\n",
    "               \\Big] \\\\\n",
    "               & + \\frac{\\lambda}{2 m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} \n",
    "               \\big( \\Theta_{j,i}^{(l)} \\big)^2\n",
    "\\end{align} \n",
    "\\\\ \n",
    "\\\\\n",
    "h_\\Theta (x) \\in \\mathbb{R}^K, (h_\\Theta(x))_i := i\\text{th output}\n",
    "$$\n",
    "\n",
    "Again, this resembles the logistic regression, where we're basically summing over multiple logistic regression cost functions. Our regularization term is similar again, but we're summing over the parameter matix (rather than array) for all of the layers.\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "\n",
    "The backpropagation algorithm is a method for minimizing the cost function on the paramters. To do so, we'll again need to compute, for a given set of parameters\n",
    "\n",
    "$$ J(\\Theta) \\text{ and } \\frac{\\partial}{\\partial \\Theta_{i,j}^{(l)}} J(\\Theta) $$\n",
    "\n",
    "Let's start by talking about a training set of only one sample,\n",
    "\n",
    "$$(x,y)$$\n",
    "\n",
    "and using a 4 layer NN. Our forward propagation looks as following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{(1)} & = x \\\\\n",
    "z^{(2)} & = \\Theta^{(1)} a^{(1)} \\\\\n",
    "a^{(2)} & = g(z^{(2)}), \\text{ add } a^{(2)}_0 \\\\\n",
    "z^{(3)} & = \\Theta^{(2)} a^{(2)} \\\\\n",
    "a^{(3)} & = g(z^{(3)}), \\text{ add } a^{(3)}_0 \\\\\n",
    "z^{(4)} & = \\Theta^{(3)} a^{(3)} \\\\\n",
    "a^{(4)} & = h_\\Theta(x) = g(z^{(4)}) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
